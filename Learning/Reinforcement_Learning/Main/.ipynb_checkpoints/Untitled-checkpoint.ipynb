{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b879a4e-122c-495a-bd26-a3efd31b219c",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6deeac51-9e0c-4934-a319-62cc8bc2028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: gymnasium==0.28.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (1.26.4)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (0.0.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3) (2.3.1+cu118)\n",
      "Requirement already satisfied: pandas in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3) (3.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13->stable-baselines3) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->stable-baselines3) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alex1\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->stable-baselines3) (2024.1)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.13->stable-baselines3) (2021.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alex1\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [1 lines of output]\n",
      "  error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (2.3.1+cu118)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (3.8.4)\n",
      "Collecting opencv-python (from stable-baselines3[extra])\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pygame in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (2.5.2)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (2.17.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\alex1\\appdata\\roaming\\python\\python312\\site-packages (from stable-baselines3[extra]) (5.9.8)\n",
      "Collecting tqdm (from stable-baselines3[extra])\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting rich (from stable-baselines3[extra])\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Using cached Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stable-baselines3[extra]) (10.3.0)\n",
      "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Using cached AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting click (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.31.0)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Using cached AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "INFO: pip is looking at multiple versions of shimmy[atari] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting stable-baselines3[extra]\n",
      "  Using cached stable_baselines3-2.3.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "  Using cached stable_baselines3-2.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "  Using cached stable_baselines3-2.1.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting shimmy~=1.1.0 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Using cached Shimmy-1.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting stable-baselines3[extra]\n",
      "  Using cached stable_baselines3-2.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting shimmy~=0.2.1 (from shimmy[atari]~=0.2.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Using cached Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting stable-baselines3[extra]\n",
      "  Using cached stable_baselines3-1.8.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting gym==0.21 (from stable-baselines3[extra])\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Requirement already satisfied: gym in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gym) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gym) (0.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "stable-baselines3 2.3.2 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0a2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3 gymnasium==0.28.1\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install gym\n",
    "!pip install shimmy>=0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3c3cf4-8e51-4f4d-93d0-bebab767b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium==0.28.1\n",
      "  Using cached gymnasium-0.28.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (1.26.4)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium==0.28.1) (0.0.4)\n",
      "Using cached gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
      "Installing collected packages: gymnasium\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 1.0.0a2\n",
      "    Uninstalling gymnasium-1.0.0a2:\n",
      "      Successfully uninstalled gymnasium-1.0.0a2\n",
      "Successfully installed gymnasium-0.28.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "shimmy 2.0.0 requires gymnasium>=1.0.0a1, but you have gymnasium 0.28.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[atari] in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[atari]) (1.26.4)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[atari]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[atari]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[atari]) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
      "  Using cached Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of shimmy[atari] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[atari])\n",
      "  Using cached Shimmy-0.2.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached Shimmy-0.1.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting gymnasium[atari]\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "  Using cached gymnasium-0.29.0-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: pip is still looking at multiple versions of shimmy[atari] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gymnasium-0.28.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "  Using cached gymnasium-0.28.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached gymnasium-0.27.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting gymnasium-notices>=0.0.1 (from gymnasium[atari])\n",
      "  Using cached gymnasium_notices-0.0.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting gymnasium[atari]\n",
      "  Using cached gymnasium-0.27.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Using cached Gymnasium-0.26.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "INFO: pip is looking at multiple versions of gymnasium[atari] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached gymnasium-0.26.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "  Using cached gymnasium-0.26.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "\n",
      "The conflict is caused by:\n",
      "    gymnasium[atari] 0.26.3 depends on ale-py~=0.8.0; extra == \"atari\"\n",
      "    gymnasium[atari] 0.26.2 depends on ale-py~=0.8.0; extra == \"atari\"\n",
      "    gymnasium[atari] 0.26.1 depends on ale-py~=0.8.0; extra == \"atari\"\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install gymnasium[atari]==0.26.1, gymnasium[atari]==0.26.2 and gymnasium[atari]==0.26.3 because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (1.64.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (70.3.0)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\alex1\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium==0.28.1\n",
    "!pip install gymnasium[atari]\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd9b131-973f-4ec8-b197-827fb2b2818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# gym is openai gym (lets work with preexisitng enivorments)\n",
    "import gym \n",
    "\n",
    "# RL algorithm PPO (more on the baseline site)\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Wrapper on baseline\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Makes it easier to test the models performance\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a99193-cc41-46f6-acbc-9b27606a7c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original method of using the dummy to display the screen was \n",
    "# unreliable so using matplotlib to show itinstead\n",
    "\n",
    "# Function to display the frame\n",
    "def display_frame(frame):\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27cfd09-1d22-4b19-8b7e-8fe568062d1e",
   "metadata": {},
   "source": [
    "# Enviroments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebeee4bb-47bc-4e19-a9b1-5e9a089e0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# A preinstalled openai enviroment\n",
    "environment_name = \"CartPole-v0\"\n",
    "\n",
    "# Choosing to use \"rgb_array\" will return 5 values, using \"human\" will return 4 values\n",
    "env = gym.make(environment_name, render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891a0d8d-9b4c-4629-a500-0a489b408245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:24.0\n",
      "Episode:2 Score:18.0\n",
      "Episode:3 Score:24.0\n",
      "Episode:4 Score:24.0\n",
      "Episode:5 Score:35.0\n"
     ]
    }
   ],
   "source": [
    "# Loop through enviroment 5 times to see how to interact with it\n",
    "# Each episode is an entire iteration of the game the agent is training on\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "\n",
    "    # env.reset is the observations for the enviroment not just the pole\n",
    "    # We will later be passing this to the agent for it to decide what action to take for the observation\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "\n",
    "        # Generate random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Pass through random action\n",
    "        # Returns observations, reward (whether or not getting a positve change \"1\" \n",
    "        # or a negative change \"0\" or \"-1\"), boolean (signfiicying whether episode is done)\n",
    "        n_state, reward, done, info, truncated = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e05f8c-d3ce-43be-910b-c157c50ded28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport subprocess\\n\\n# Install the required packages using subprocess\\nsubprocess.run([\\'apt-get\\', \\'install\\', \\'-y\\', \\'xvfb\\'])\\nsubprocess.run([\\'pip\\', \\'install\\', \\'stable-baselines3[extra]\\'])\\nsubprocess.run([\\'pip\\', \\'install\\', \\'gym\\', \\'moviepy\\', \\'pyvirtualdisplay\\'])\\n\\nimport gym\\nimport moviepy.editor as mpy\\nimport numpy as np\\nfrom IPython.display import Video\\nfrom pyvirtualdisplay import Display\\nimport matplotlib.pyplot as plt\\n\\n# Function to create video from frames\\ndef create_video(frames, video_name):\\n    clip = mpy.ImageSequenceClip(frames, fps=30)\\n    clip.write_videofile(video_name, codec=\\'libx264\\')\\n\\n# Initialize the virtual display\\ndisplay = Display(visible=0, size=(1400, 900))\\ndisplay.start()\\n\\n# Initialize the environment\\nenv = gym.make(\\'CartPole-v1\\')\\n\\n# Loop through environment 5 times to see how to interact with it\\n# Each episode is an entire iteration of the game the agent is training on\\nepisodes = 5\\nall_frames = []  # To store frames for all episodes\\nfor episode in range(1, episodes+1):\\n\\n    # env.reset is the observations for the environment not just the pole\\n    # We will later be passing this to the agent for it to decide what action to take for the observation\\n    state = env.reset()\\n    done = False\\n    score = 0 \\n\\n    # *\\n    frames = []\\n\\n    while not done:\\n        # *\\n        # Code below is originally env.render()\\n        # Capture the frame for the video\\n        frame = env.render(mode=\\'rgb_array\\')\\n        frames.append(frame)\\n        \\n        # Generate random action\\n        action = env.action_space.sample()\\n        \\n        # Pass through random action\\n        # Returns observations, reward (whether or not getting a positive change \"1\" \\n        # or a negative change \"0\" or \"-1\"), boolean (signifying whether episode is done)\\n        n_state, reward, done, info = env.step(action)\\n        score += reward\\n    \\n    print(\\'Episode:{} Score:{}\\'.format(episode, score))\\n    all_frames.extend(frames)\\n\\nenv.close()\\n\\n# *\\n# Create the video from the frames captured\\ncreate_video(all_frames,\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import subprocess\n",
    "\n",
    "# Install the required packages using subprocess\n",
    "subprocess.run(['apt-get', 'install', '-y', 'xvfb'])\n",
    "subprocess.run(['pip', 'install', 'stable-baselines3[extra]'])\n",
    "subprocess.run(['pip', 'install', 'gym', 'moviepy', 'pyvirtualdisplay'])\n",
    "\n",
    "import gym\n",
    "import moviepy.editor as mpy\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "from pyvirtualdisplay import Display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create video from frames\n",
    "def create_video(frames, video_name):\n",
    "    clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "    clip.write_videofile(video_name, codec='libx264')\n",
    "\n",
    "# Initialize the virtual display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Loop through environment 5 times to see how to interact with it\n",
    "# Each episode is an entire iteration of the game the agent is training on\n",
    "episodes = 5\n",
    "all_frames = []  # To store frames for all episodes\n",
    "for episode in range(1, episodes+1):\n",
    "\n",
    "    # env.reset is the observations for the environment not just the pole\n",
    "    # We will later be passing this to the agent for it to decide what action to take for the observation\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "\n",
    "    # *\n",
    "    frames = []\n",
    "\n",
    "    while not done:\n",
    "        # *\n",
    "        # Code below is originally env.render()\n",
    "        # Capture the frame for the video\n",
    "        frame = env.render(mode='rgb_array')\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Generate random action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Pass through random action\n",
    "        # Returns observations, reward (whether or not getting a positive change \"1\" \n",
    "        # or a negative change \"0\" or \"-1\"), boolean (signifying whether episode is done)\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "    all_frames.extend(frames)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# *\n",
    "# Create the video from the frames captured\n",
    "create_video(all_frames,\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc11ec-8949-42d7-888f-1b672da5578d",
   "metadata": {},
   "source": [
    "# Understanding the Enviroment\n",
    "\n",
    "There are two parts to our enviroment the action space and the observation space\n",
    "\n",
    "This site shows what each value from the action_space and observation space mean https://www.gymlibrary.dev/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0114e367-2f1b-44f8-9d11-ed8f2f3e46e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows type \n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed64f4d3-d9e5-4919-aed0-4888c738457b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows example \n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51031403-adbb-4881-8dbb-483cd9d61288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows type \n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6941806b-ab59-4918-a840-3e21958bf4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.4851182e+00,  2.1598194e+38, -9.5480695e-02,  4.3306636e+37],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows example \n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e204cb-af60-4eae-a49b-618fa235423a",
   "metadata": {},
   "source": [
    "# Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0352b6d0-7895-4489-b66e-25ad5c3c0208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your directories here\n",
    "log_path = os.path.join('Training', 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aa7f855-e1f2-4e94-9200-fc492b43c2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\logs'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1489dd28-9d0c-4165-8acf-6c51c72ba019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Create the enviroment (Same as previous enviroment creation)\n",
    "env = gym.make(environment_name)\n",
    "\n",
    "# Wrap enviroment around dummy enviroment (wrapper for non-vecotrized enviroment)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Define PPO Model \n",
    "# Define Policy (Neural Network policy)\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "612e6f5a-15de-4bcf-8ef9-71aba2aea89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpolicy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mActorCriticPolicy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0menv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgymnasium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VecEnv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0003\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_epochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgae_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclip_range\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mclip_range_vf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnormalize_advantage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0ment_coef\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvf_coef\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muse_sde\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msde_sample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrollout_buffer_class\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRolloutBuffer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrollout_buffer_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtarget_kl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstats_window_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m        \n",
       "\u001b[1;32mclass\u001b[0m \u001b[0mPPO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOnPolicyAlgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"\n",
       "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
       "\n",
       "    Paper: https://arxiv.org/abs/1707.06347\n",
       "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
       "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
       "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
       "\n",
       "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
       "\n",
       "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
       "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
       "    :param learning_rate: The learning rate, it can be a function\n",
       "        of the current progress remaining (from 1 to 0)\n",
       "    :param n_steps: The number of steps to run for each environment per update\n",
       "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
       "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
       "        See https://github.com/pytorch/pytorch/issues/29372\n",
       "    :param batch_size: Minibatch size\n",
       "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
       "    :param gamma: Discount factor\n",
       "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
       "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
       "        remaining (from 1 to 0).\n",
       "    :param clip_range_vf: Clipping parameter for the value function,\n",
       "        it can be a function of the current progress remaining (from 1 to 0).\n",
       "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
       "        no clipping will be done on the value function.\n",
       "        IMPORTANT: this clipping depends on the reward scaling.\n",
       "    :param normalize_advantage: Whether to normalize or not the advantage\n",
       "    :param ent_coef: Entropy coefficient for the loss calculation\n",
       "    :param vf_coef: Value function coefficient for the loss calculation\n",
       "    :param max_grad_norm: The maximum value for the gradient clipping\n",
       "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
       "        instead of action noise exploration (default: False)\n",
       "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
       "        Default: -1 (only sample at the beginning of the rollout)\n",
       "    :param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
       "    :param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation\n",
       "    :param target_kl: Limit the KL divergence between updates,\n",
       "        because the clipping is not enough to prevent large update\n",
       "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
       "        By default, there is no limit on the kl div.\n",
       "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
       "        the reported success rate, mean episode length, and mean reward over\n",
       "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
       "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
       "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
       "        debug messages\n",
       "    :param seed: Seed for the pseudo random generators\n",
       "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
       "        Setting it to auto, the code will be run on the GPU if possible.\n",
       "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpolicy_aliases\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mClassVar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBasePolicy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"MlpPolicy\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mActorCriticPolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"CnnPolicy\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mActorCriticCnnPolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"MultiInputPolicy\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMultiInputActorCriticPolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mpolicy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mActorCriticPolicy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0menv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGymEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSchedule\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mn_steps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mn_epochs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgamma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgae_lambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mclip_range\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSchedule\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mclip_range_vf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSchedule\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mnormalize_advantage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0ment_coef\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mvf_coef\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0muse_sde\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msde_sample_freq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mrollout_buffer_class\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mType\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRolloutBuffer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mrollout_buffer_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtarget_kl\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstats_window_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mseed\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mgae_lambda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgae_lambda\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0ment_coef\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ment_coef\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mvf_coef\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvf_coef\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0muse_sde\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_sde\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0msde_sample_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msde_sample_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mrollout_buffer_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrollout_buffer_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mrollout_buffer_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrollout_buffer_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mstats_window_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstats_window_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mtensorboard_log\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensorboard_log\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0msupported_action_spaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDiscrete\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiBinary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# because of the advantage normalization\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mnormalize_advantage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mbatch_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# Check that `n_steps * n_envs > 1` to avoid NaN\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# when doing advantage normalization\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mbuffer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32massert\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mnot\u001b[0m \u001b[0mnormalize_advantage\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33mf\"\u001b[0m\u001b[1;33m`n_steps * n_envs` must be greater than 1. Currently n_steps=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m and n_envs=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# Check that the rollout buffer size is a multiple of the mini-batch size\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0muntruncated_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mbuffer_size\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33mYou have specified a mini-batch size of \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33m but because the `RolloutBuffer` is of size `n_steps * n_envs = \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m`,\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33m after every \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0muntruncated_batches\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m untruncated mini-batches,\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33m there will be a truncated mini-batch of size \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mbuffer_size\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\\n\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33mWe recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33mf\"\u001b[0m\u001b[1;33mInfo: (n_steps=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m and n_envs=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip_range_vf\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_advantage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_advantage\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_kl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_kl\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Initialize schedules for policy/value clipping\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_schedule_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"`clip_range_vf` must be positive, \"\u001b[0m \u001b[1;34m\"pass `None` to deactivate vf clipping\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_schedule_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"\n",
       "        Update policy using the currently gathered rollout buffer.\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Switch to train mode (this affects batch norm / dropout)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_training_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Update optimizer learning rate\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_learning_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Compute current clip range\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mclip_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_progress_remaining\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[operator]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Optional: clip range for the value function\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mclip_range_vf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_progress_remaining\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[operator]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mentropy_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mpg_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mclip_fractions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# train for n_epochs epochs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mapprox_kl_divs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# Do a complete pass on the rollout buffer\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mrollout_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# Convert discrete action from float to long\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Re-sample the noise matrix because the log_std has changed\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_sde\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Normalize advantage\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvantages\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Normalization does not make sense if mini batchsize == 1, see GH issue #325\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_advantage\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0madvantages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0madvantages\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# ratio between old and new policy, should be one at the first iteration\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_prob\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold_log_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# clipped surrogate loss\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mpolicy_loss_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mpolicy_loss_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madvantages\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_loss_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Logging\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mpg_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mclip_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mclip_fractions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_fraction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# No clipping\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mvalues_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# Clip the difference between old and new value\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# NOTE: this depends on the reward scaling\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mvalues_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold_values\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mvalues\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mclip_range_vf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_range_vf\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Value loss using the TD(gae_lambda) target\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mvalue_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mvalue_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Entropy loss favor exploration\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;31m# Approximate entropy when no analytical form\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mentropy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mentropy_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mentropy_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentropy_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ment_coef\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mentropy_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvf_coef\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mvalue_loss\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Calculate approximate form of reverse KL Divergence for early stopping\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# and Schulman blog: http://joschu.net/blog/kl-approx.html\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mlog_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mold_log_prob\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mapprox_kl_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_ratio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlog_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mapprox_kl_divs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapprox_kl_div\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_kl\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mapprox_kl_div\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_kl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mEarly stopping at step \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m due to reaching max kl: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mapprox_kl_div\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m.2f\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mbreak\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Optimization step\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;31m# Clip grad norm\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_updates\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mbreak\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mexplained_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplained_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# Logs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/entropy_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentropy_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/policy_gradient_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpg_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/value_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/approx_kl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapprox_kl_divs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/clip_fraction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_fractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/explained_variance\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplained_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"log_std\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/std\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/n_updates\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_updates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tensorboard\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/clip_range\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_range_vf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train/clip_range_vf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_range_vf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSelfPPO\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMaybeCallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mlog_interval\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtb_log_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"PPO\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mprogress_bar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSelfPPO\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See all arguments you can pass and what hyperparameters you can change\n",
    "PPO??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "282ee21e-0b25-4dc2-b0bc-cb6fdc9c251d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\logs\\PPO_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 409  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 4    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008530549 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00209     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.43        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 51          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 310         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012191741 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 265         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009727918 |\n",
      "|    clip_fraction        | 0.0884      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 53.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007664253 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 65          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008229396 |\n",
      "|    clip_fraction        | 0.0882      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 61.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 201          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086444905 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.585       |\n",
      "|    explained_variance   | 0.516        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.4         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00937     |\n",
      "|    value_loss           | 51.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042821886 |\n",
      "|    clip_fraction        | 0.0345       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.66         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0053      |\n",
      "|    value_loss           | 32.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 189          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085009895 |\n",
      "|    clip_fraction        | 0.0619       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.12         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00729     |\n",
      "|    value_loss           | 66.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 185          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029855266 |\n",
      "|    clip_fraction        | 0.0232       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.5         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00458     |\n",
      "|    value_loss           | 60.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17f553b2fc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowered from 20000 to 200 for testing\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca1a80-7a20-4c6b-9bfa-ff9d9e87215b",
   "metadata": {},
   "source": [
    "# Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284e0a19-23b2-4a4d-9936-54cdfafcd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved_Models', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73892866-2935-4025-a5cb-9d3b3780519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0834295-efcc-4bce-8013-58195140b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleteing model to learn how to call upon a saved model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "268d86e4-6792-4925-b85f-c0293a8cbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbfdf3c-7f7a-4e92-b8c9-79b3c81eecb8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097e00a9-2e20-4be6-886c-118f1c875ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9708710-2996-4802-8757-a3e2816e0f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the policy\n",
    "# passing the enviroment, the enviroment, how many episodes we want it to test on, we can see it \n",
    "\n",
    "# The output is a tuple: (x, y)\n",
    "# The x is average reward, and the y is the standard deviation\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01439368-c854-421e-b28c-dc11cd9955e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d66bf-295b-4a54-8e5f-dd184cf6f3fd",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92fd5fc1-1653-462a-8752-b99b07093800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79038856-b399-4a40-ba9d-84b26804542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1], dtype=int64), None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd32b1b0-1f6b-4e8d-9b33-7bfcebcbaa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alex1\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alex1\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alex1\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68a9ad8c-9e7a-41de-9738-ef2b6c1bc2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# The code below was supposed to work, but there were some errors and we swtiched to using matplotlib\\n# To graph the progress\\n\\nobs = env.reset()\\nwhile True:\\n    action, _states = model.predict(obs)\\n    obs, rewards, done, info = env.step(action)\\n    env.render()\\n    if done: \\n        print('info', info)\\n        break\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# The code below was supposed to work, but there were some errors and we swtiched to using matplotlib\n",
    "# To graph the progress\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc807f40-d129-41e9-b343-61e99c95c678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[200.]\n",
      "Episode:2 Score:[200.]\n",
      "Episode:3 Score:[200.]\n",
      "Episode:4 Score:[200.]\n",
      "Episode:5 Score:[200.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    # env.reset() gets the observations from our observation space\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53f0d6e7-c7d7-4786-a99f-72f9303ab7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict returns the model action and the next state\n",
    "# The ?? lets us know more about the call\n",
    "# model.predict??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea7862c2-f16b-4f3d-9621-68d16e96018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1fea9a9-c4b8-4eda-bbac-8a6ce28e5635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02857458, -0.02538106, -0.02882311, -0.02163134]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obs (from documentation) contains 4 values (the cart position, cart velocity, pole angle, pole velocity\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "222c37db-9f7c-45df-96b1-e757272d8833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1], dtype=int64), None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is basically saying that in that current enviroment, its best to choose option 1\n",
    "# For the best possible reward\n",
    "model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af111a2c-2acd-40c3-a8dc-12d96dc8c502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method just calls a random action from the action space (just to see what options there are\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40e7e31d-cdf6-405c-816e-93ae47b02cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The code below is to further explain what happens during the model testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e393ab62-2258-40cf-b83f-68df16656cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d02ead3-f35a-4724-be65-fbdbbb7d37cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.02908221, -0.22007805, -0.02925574,  0.26182002]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{'TimeLimit.truncated': False}])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is where the agent receives a reward\n",
    "# The array is the state of the pole and cart after the action is taken\n",
    "# The second array is the reward (in the case of running its a 1) \n",
    "# For every step taken the agent recieves a reward as long as the pole and cart are within the\n",
    "# Specified Threshold, otherwise it gets a 0 (no reward)\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1ba84-abd3-48fe-b205-49531f8006cd",
   "metadata": {},
   "source": [
    "# Tensorboards\n",
    "\n",
    "C:\\Users\\alex1\\Desktop\\Ahmad_Stuff\\Random\\Fetched_Repositories\\Learning\\Learning\\Learning\\Reinforcement_Learning\\Main\\.ipynb_checkpoints\\Training\\Logs\n",
    "\n",
    "Search http://localhost:6006/ in the searchbar to view the tensorboard output (graphs of the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "115fa1e2-d0e7-4ea4-8027-33e4dea19ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "482fbf25-5a69-4964-89cc-414112bda615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dabe98-3731-4b62-be6a-4fb0e5540e09",
   "metadata": {},
   "source": [
    "# Adding Callback to Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b938e8b-3236-461e-8bd8-6957e2acab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EvalCallBack will be running throughout the training phase, and StopTraining... will\n",
    "# check if it achieves the desired score\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b8d86443-b8eb-476d-a28b-4ff9607f04ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved_Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fd070f3-4cea-4fb6-8cb9-55ab4feab2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set when to stop (here 200)\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
    "\n",
    "# This will be trigered after each training run\n",
    "# First Parameter: Passing enviroment, \n",
    "# Second parameter: stop_callback wil be triggered when a new best evaluation is achieved, \n",
    "# Third parameter: the agent will be evaluated every 10,0000 steps\n",
    "# Fourth parameter: where to save the best model\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b4827f4-98e4-4655-ab8d-13864f214d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7cf1aa14-3f25-47a5-9bf4-5d2078096913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 255  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 8    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 199         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008320896 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.000414    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.84        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 52.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008575628 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.0462      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 44.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008724142 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 61.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009427982 |\n",
      "|    clip_fraction        | 0.0798      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33          |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    value_loss           | 74.1        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 200.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17f6129e540>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are passing callback here\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2cfb6-22ff-42ad-bd12-1a54ca8c365c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
